{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DIgM6C9HYUhm"
      },
      "source": [
        "# Context-sensitive Spelling Correction\n",
        "\n",
        "The goal of the assignment is to implement context-sensitive spelling correction. The input of the code will be a set of text lines and the output will be the same lines with spelling mistakes fixed.\n",
        "\n",
        "Submit the solution of the assignment to Moodle as a link to your GitHub repository containing this notebook.\n",
        "\n",
        "Useful links:\n",
        "- [Norvig's solution](https://norvig.com/spell-correct.html)\n",
        "- [Norvig's dataset](https://norvig.com/big.txt)\n",
        "- [Ngrams data](https://www.ngrams.info/download_coca.asp)\n",
        "\n",
        "Grading:\n",
        "- 60 points - Implement spelling correction\n",
        "- 20 points - Justify your decisions\n",
        "- 20 points - Evaluate on a test set\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-vb8yFOGRDF"
      },
      "source": [
        "## Implement context-sensitive spelling correction\n",
        "\n",
        "Your task is to implement context-sensitive spelling corrector using N-gram language model. The idea is to compute conditional probabilities of possible correction options. For example, the phrase \"dking sport\" should be fixed as \"doing sport\" not \"dying sport\", while \"dking species\" -- as \"dying species\".\n",
        "\n",
        "The best way to start is to analyze [Norvig's solution](https://norvig.com/spell-correct.html) and [N-gram Language Models](https://web.stanford.edu/~jurafsky/slp3/3.pdf).\n",
        "\n",
        "When solving this task, we expect you'll face (and successfully deal with) some problems or make up the ideas of the model improvement. Some of them are:\n",
        "\n",
        "- solving a problem of n-grams frequencies storing for a large corpus;\n",
        "- taking into account keyboard layout and associated misspellings;\n",
        "- efficiency improvement to make the solution faster;\n",
        "- ...\n",
        "\n",
        "Please don't forget to describe such cases, and what you decided to do with them, in the Justification section.\n",
        "\n",
        "##### IMPORTANT:  \n",
        "Your project should not be a mere code copy-paste from somewhere. You must provide:\n",
        "- Your implementation\n",
        "- Analysis of why the implemented approach is suggested\n",
        "- Improvements of the original approach that you have chosen to implement"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Improved approach (my solution)"
      ],
      "metadata": {
        "id": "cHOmhdt2Ps7u"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UqmhqFrIatW-",
        "outputId": "2a73c0fd-59bd-42a8-a72d-868862e296cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "import nltk\n",
        "from collections import Counter\n",
        "from nltk.util import ngrams\n",
        "from difflib import get_close_matches\n",
        "import re\n",
        "from itertools import product\n",
        "import string\n",
        "import random\n",
        "nltk.download('punkt_tab')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "LwGJOFzycxWX"
      },
      "outputs": [],
      "source": [
        "def preprocess(data):\n",
        "  tokens = nltk.word_tokenize(data.lower())\n",
        "  return [token for token in tokens if token.isalpha()]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "qwerty_neighbors = {\n",
        "    'a': \"qwsz\", 'b': \"vghn\", 'c': \"xdfv\", 'd': \"ersfxc\",\n",
        "    'e': \"rdsw\", 'f': \"rtgvcd\", 'g': \"tyhbfv\", 'h': \"yujnbg\",\n",
        "    'i': \"uojk\", 'j': \"uikmnh\", 'k': \"iolmj\", 'l': \"opk\",\n",
        "    'm': \"njk\", 'n': \"bhjm\", 'o': \"iplk\", 'p': \"ol\",\n",
        "    'q': \"wa\", 'r': \"etdf\", 's': \"awedxz\", 't': \"rygf\",\n",
        "    'u': \"yihj\", 'v': \"cfgb\", 'w': \"qase\", 'x': \"zsdc\",\n",
        "    'y': \"tuhg\", 'z': \"asx\"\n",
        "}\n",
        "\n",
        "# Function to get substitution weight based on keyboard proximity\n",
        "def key_distance(c1, c2):\n",
        "    \"\"\"Return a weight factor based on keyboard proximity.\"\"\"\n",
        "    if c1 == c2:\n",
        "        return 1.0  # No substitution\n",
        "    elif c2 in qwerty_neighbors.get(c1, ''):\n",
        "        return 1.2  # Close key, lower penalty\n",
        "    else:\n",
        "        return 0.8"
      ],
      "metadata": {
        "id": "1BuxkTUvEs_k"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# receives a pair of words and receives the corrected one\n",
        "def correct_bigram(words):\n",
        "  if len(words) != 2:\n",
        "      print(\"two words expected\")\n",
        "      return\n",
        "  cands = []\n",
        "  for word in words:\n",
        "      cands.append(candidates(word))\n",
        "  i,j = cands\n",
        "  #print(i,\"-------\",j)\n",
        "  permutations = [(x, y) for x, y in product(i, j)]\n",
        "  return max(permutations, key = bigramProbability)\n",
        "\n",
        "def correct_single(word):\n",
        "  cands = candidates(word)\n",
        "  return max(cands, key = singleProbability)"
      ],
      "metadata": {
        "id": "TmIP_EIhOZjw"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "CqT-le8wnOYy"
      },
      "outputs": [],
      "source": [
        "# calculates the probability for each bigram, taking edit distance int account\n",
        "def bigramProbability(words):\n",
        "    if len(words) != 2:\n",
        "      print(\"two words expected\")\n",
        "      return\n",
        "    bigram = (words[0][0],words[1][0])\n",
        "    return (bg[bigram]/words_count) * words[0][1] * words[1][1]\n",
        "# calculates the probability for single word\n",
        "def singleProbability(word):\n",
        "    global word_freq,words_count\n",
        "    return (words_freq[word[0]]/words_count) * word[1]\n",
        "\n",
        "# return candidates for word correction\n",
        "def candidates(word):\n",
        "    res = edits1(word,pas = 1)\n",
        "    return (known([(word,1)]) or known(res) or known(edits2(res)) or {(word,1)})\n",
        "\n",
        "# check if the word is valid\n",
        "def known(words):\n",
        "    return set(w for w in words if w[0] in words_freq)\n",
        "\n",
        "# 1 edit corrections\n",
        "def edits1(word,pas): #called for each word separatly\n",
        "    letters    = 'abcdefghijklmnopqrstuvwxyz'\n",
        "\n",
        "    if pas == 1:\n",
        "      splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
        "      deletes    = [(L + R[1:],0.8)               for L, R in splits if R] #deletes are fined by 0.8 since its less likely in my practice\n",
        "      transposes = [(L + R[1] + R[0] + R[2:],1) for L, R in splits if len(R)>1] #transposes are left with weight 1, since they are pretty common, considered as base\n",
        "      replaces   = [(L + c + R[1:],key_distance(R[0], c))           for L, R in splits if R for c in letters] # all replaces find by keyboard layout, from 0.8 to 1.2, so its the only one what may have positive impact on the probability, since if the latter is really close, it more likely to be just a typo\n",
        "      inserts    = [(L + c + R,0.8)               for L, R in splits for c in letters] # all insertions fined by 0.8 similar to deletes\n",
        "      if word == \"rotted\":\n",
        "        print(replaces)\n",
        "    elif pas == 2:                                                                  #second pass\n",
        "      splits     = [(word[0][:i], word[0][i:])    for i in range(len(word[0]) + 1)]\n",
        "      deletes    = [(L + R[1:],word[1]*0.8)               for L, R in splits if R]\n",
        "      transposes = [(L + R[1] + R[0] + R[2:],word[1]*1) for L, R in splits if len(R)>1]\n",
        "      replaces   = [(L + c + R[1:],word[1]*key_distance(R[0], c))           for L, R in splits if R for c in letters]\n",
        "      inserts    = [(L + c + R,word[1]*0.8)           for L, R in splits for c in letters]\n",
        "    return set(deletes + transposes + replaces + inserts)\n",
        "\n",
        "# 2 edit corrections\n",
        "def edits2(res):\n",
        "    a = {e2 for e1 in res for e2 in edits1(e1,pas = 2)}\n",
        "    return a"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Helper functions to process text and pass it to solution"
      ],
      "metadata": {
        "id": "c4OOmsoFP7Cs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# receives a text, divide it on bigrams and corrects the text\n",
        "def fix_typos(text):\n",
        "    # split text into words and punctuation while preserving spaces\n",
        "    tokens = re.findall(r\"\\w+\\'\\w+|\\w+|[.,!?;]\", text.lower())\n",
        "\n",
        "    fixed_tokens = []\n",
        "    for i in range(len(tokens) - 1):\n",
        "        word1, word2 = tokens[i], tokens[i + 1]\n",
        "\n",
        "        # ensure both are words (not punctuation)\n",
        "        if word1.isalpha() and word2.isalpha():\n",
        "            corrected = correct_bigram([word1, word2])\n",
        "            if corrected:\n",
        "                fixed_tokens.append(corrected[0][0])  # first word from corrected bigram\n",
        "                tokens[i + 1] = corrected[1][0]  # replace next word directly in tokens\n",
        "                continue  # Skip appending word1 again\n",
        "        elif word1.isalpha() and not word2.isalpha():\n",
        "            corrected = correct_single(word1)\n",
        "            if corrected:\n",
        "                fixed_tokens.append(corrected[0])  # extract only the word\n",
        "            else:\n",
        "                fixed_tokens.append(word1)\n",
        "        elif not word1.isalpha() and word2.isalpha():\n",
        "            corrected = correct_single(word2)\n",
        "            if corrected:\n",
        "                tokens[i + 1] = corrected[0]  # extract only the word\n",
        "            fixed_tokens.append(word1)\n",
        "        else:\n",
        "            fixed_tokens.append(word1)\n",
        "\n",
        "    # Append the last token\n",
        "    fixed_tokens.append(tokens[-1])\n",
        "    fixed_text = \" \".join(fixed_tokens)\n",
        "    fixed_text = re.sub(r\"\\s+([{}])\".format(re.escape(string.punctuation)), r\"\\1\", fixed_text)\n",
        "    return fixed_text"
      ],
      "metadata": {
        "id": "hM0C1yls8vSu"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# compares two texts and prints mismatches\n",
        "def compare_text(correct, text2, show_mismatches=False):\n",
        "    def preprocess(text):\n",
        "        \"\"\"Tokenizes, lowercases, and removes punctuation from text.\"\"\"\n",
        "        return [word.lower().strip(string.punctuation) for word in text.split()]\n",
        "\n",
        "    correct_words = preprocess(correct)\n",
        "    text2_words = preprocess(text2)\n",
        "\n",
        "    matches = 0\n",
        "    mismatches = []\n",
        "\n",
        "    for c, t in zip(correct_words, text2_words):\n",
        "        if c == t:\n",
        "            matches += 1\n",
        "        else:\n",
        "            mismatches.append((c, t,))\n",
        "\n",
        "    total_words = max(len(correct_words), len(text2_words))\n",
        "    accuracy = matches / total_words if total_words > 0 else 0\n",
        "\n",
        "    print(f\"Correct words: {matches}/{total_words}\")\n",
        "    print(f\"Accuracy: {accuracy:.2%}\")\n",
        "\n",
        "    if show_mismatches and mismatches:\n",
        "        print(\"\\nMismatched words:\")\n",
        "        for correct_word, test_word in mismatches:\n",
        "            print(f\"Expected: '{correct_word}' → Found: '{test_word}', \")"
      ],
      "metadata": {
        "id": "xkXQ_qw0zjUk"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Norvig Solution"
      ],
      "metadata": {
        "id": "Ant-lN5oQFBG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from collections import Counter\n",
        "\n",
        "def to_words(text): return re.findall(r'\\w+', text.lower())\n",
        "\n",
        "words_counter = Counter(to_words(open('big.txt').read()))\n",
        "\n",
        "def P(word, N=sum(words_counter.values())):\n",
        "    \"Probability of `word`.\"\n",
        "    return words_counter[word] / N\n",
        "\n",
        "def correctionN(word):\n",
        "    \"Most probable spelling correction for word.\"\n",
        "    return max(candidatesN(word), key=P)\n",
        "\n",
        "def candidatesN(word):\n",
        "    \"Generate possible spelling corrections for word.\"\n",
        "    return (knownN([word]) or knownN(edits1N(word)) or knownN(edits2N(word)) or [word])\n",
        "\n",
        "def knownN(words):\n",
        "    \"The subset of `words` that appear in the dictionary of WORDS.\"\n",
        "    return set(w for w in words if w in words_counter)\n",
        "\n",
        "def edits1N(word):\n",
        "    \"All edits that are one edit away from `word`.\"\n",
        "    letters    = 'abcdefghijklmnopqrstuvwxyz'\n",
        "    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
        "    deletes    = [L + R[1:]               for L, R in splits if R]\n",
        "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
        "    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n",
        "    inserts    = [L + c + R               for L, R in splits for c in letters]\n",
        "    return set(deletes + transposes + replaces + inserts)\n",
        "\n",
        "def edits2N(word):\n",
        "    \"All edits that are two edits away from `word`.\"\n",
        "    return (e2 for e1 in edits1N(word) for e2 in edits1N(e1))\n",
        "\n",
        "def Norvig_fix_typos(text):\n",
        "    words_in_text = re.findall(r'\\w+\\'\\w+|\\w+', text.lower())\n",
        "    corrected_words = [correctionN(word) for word in words_in_text]\n",
        "\n",
        "    return ' '.join(corrected_words)"
      ],
      "metadata": {
        "id": "Q7UqVMQ5QEB4"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oML-5sJwGRLE"
      },
      "source": [
        "## Justify your decisions\n",
        "\n",
        "Write down justificaitons for your implementation choices. For example, these choices could be:\n",
        "- Which ngram dataset to use\n",
        "- Which weights to assign for edit1, edit2 or absent words probabilities\n",
        "- Beam search parameters\n",
        "- etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Xb_twOmVsC6"
      },
      "source": [
        "I used big.txt text to create a set of words and bigrams, the one from useful links. It gives me 371000 bigrams, not very big number, during testing i discovered that large number of test cases are not covered. I suppose even if dataset will be several times larger, the problem will remain. So, to test my improvements and compaer it to Norvig's solution, i will use text from big.txt with some number of intended typos.\n",
        "\n",
        "\n",
        "\n",
        "In my solution i used bigram Counter along with weighted edit distance to get the probability for pair of words. With this slight modifications, bigram approach still process combiantions of just two words at a time, so the complexity remains manageable and doesn't escalate too much. At the same time, weighted edit distance will be more effective for natural typos, since it takes into account keyboard layoyt and some other minor things(different cost for replace,insertion etc). For edit2 i multiply probability of edit1 by probability of current typo type, so two typos in the word are less probable."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46rk65S4GRSe"
      },
      "source": [
        "## Evaluate on a test set\n",
        "\n",
        "Your task is to generate a test set and evaluate your work. You may vary the noise probability to generate different datasets with varying compexity (or just take another dataset). Compare your solution to the Norvig's corrector, and report the accuracies."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generating test sets"
      ],
      "metadata": {
        "id": "aax6O1nzRlLR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "OwZWaX9VVs7B"
      },
      "outputs": [],
      "source": [
        "def introduce_typos(text, error_rate=0.1):\n",
        "    words = re.findall(r'\\w+|\\s+|[.,!?;]', text)\n",
        "    typo_text = []\n",
        "\n",
        "    for word in words:\n",
        "        if word.isalpha() and random.random() < error_rate:  # errors randomly\n",
        "            typo_text.append(make_typo(word))\n",
        "        else:\n",
        "            typo_text.append(word)\n",
        "\n",
        "    return \"\".join(typo_text)\n",
        "\n",
        "def make_typo(word):\n",
        "    typo_type = random.choice([\"insert\", \"delete\", \"swap\"])\n",
        "    index = random.randint(0, len(word) - 1)\n",
        "\n",
        "    if typo_type == \"insert\":\n",
        "        letter = random.choice(\"abcdefghijklmnopqrstuvwxyz\")\n",
        "        return word[:index] + letter + word[index:]\n",
        "    elif typo_type == \"delete\" and len(word) > 1:\n",
        "        return word[:index] + word[index + 1:]\n",
        "    elif typo_type == \"swap\" and len(word) > 1:\n",
        "        if index == len(word) - 1:\n",
        "            index -= 1\n",
        "        return word[:index] + word[index + 1] + word[index] + word[index + 2:]\n",
        "    return word\n",
        "\n",
        "# read text and generate outputs\n",
        "def generate_texts(file_path, size, error_rate=0.2):\n",
        "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        data = f.read()\n",
        "\n",
        "    # remove sonething like \"#--, \\n\"\n",
        "    data = re.sub(r'[#-]+|\\n+', ' ', data).strip()\n",
        "    data = ' '.join(word for word in data.split() if word.isalpha())\n",
        "\n",
        "    if len(data) <= size:\n",
        "        selected_text = data\n",
        "    else:\n",
        "        start = random.randint(0, len(data) - size)\n",
        "        selected_text = data[start:start + size]\n",
        "\n",
        "    good_text = selected_text\n",
        "    bad_text = introduce_typos(selected_text, error_rate)\n",
        "\n",
        "    return good_text, bad_text\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "f = open(\"big.txt\", \"r\")\n",
        "data = f.read()\n",
        "tokens = preprocess(data)\n",
        "words_freq = Counter(tokens)\n",
        "bg = Counter(ngrams(tokens,2))\n",
        "words_count = len(tokens)"
      ],
      "metadata": {
        "id": "QxjzKmu0RqG6"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "My handwritten test sample, i typed text by myslef very fast, so there is plenty **natural** typos"
      ],
      "metadata": {
        "id": "TtEINOjERsKw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "good_text = \"Not to be outdone by the mariners of the deep, western souts searched for overland routes to the Pacific. Zebulon Pike, explorer and pathfinder, by his expedition into the Southwest during Jefferson's administration, had discovered the resources of New Spain and had shown his countrymen how easy it was to reach Santa Fe from the upper waters of the Arkansas River. Not long afterward, traders laid open the route, making Franklin, Missouri, and later Fort Leavenworth the starting point. Along the trail, once surveyed, poured caravans heavily guarded by armed men against marauding Indians. Sand storms often wiped out all signs of the route; hunger and thirst did many a band of wagoners to death; but the lure of the game and the profits at the end kept the business thriving. Huge stocks of cottons, glass, hardware, and ammunition were drawn almost across the continent to be exchanged at Santa Fe for furs, Indian blankets, silver, and mules; and many a fortune was made out of the traffic.\"\n",
        "bad_text = \"Not to be outdone bu tht marines of te dep, wastent scouts searched for overland routes to the pacifiv. Zebulon Pike, explorer and oathdinder, by his expedition into the SOuthwest during Jefferson's administration, had discovered the resources of New Spain and had SHown jis countrymen haw easy it was to reafch santa fe from the upepr waters fo the arkansas river. Not lonag afereward, traders laid opne the route, markin Franklin, Missousuri, and later fort Leavenworth the starinf pount. Along ht train, once survevyed, poursed caravans heavely guarded by armed men againt marauding Indians. Sand storms often wiped out all signs of th route; unger and thirt did many a band of wgoners to death; bat the lure of the game adn the progits at the end kept the business thrivign. hife stocks of cottonfs, galss, hardwate, und ammunitaion, wer drawn almsor across the contient to be exchahnged at Santa fe for firs, Indian balnkets, silver, and miles; and many a fortune was amde out of the traffic.\"\n",
        "\n",
        "print(\"orogonal accuracy: \", end = \"\")\n",
        "compare_text(good_text,bad_text)\n",
        "print(\"\\nsolution accuracy: \",end = \"\")\n",
        "corrected_text = fix_typos(bad_text)\n",
        "compare_text(good_text,corrected_text)\n",
        "print(\"\\nNorvig accuracy: \",end = \"\")\n",
        "norvig_text = Norvig_fix_typos(bad_text)\n",
        "compare_text(good_text,norvig_text)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JSKB7teERuKc",
        "outputId": "1110dad2-c758-4c7f-886b-62ba8a5d6ecc"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "orogonal accuracy: Correct words: 119/168\n",
            "Accuracy: 70.83%\n",
            "\n",
            "solution accuracy: Correct words: 149/168\n",
            "Accuracy: 88.69%\n",
            "\n",
            "Norvig accuracy: Correct words: 142/168\n",
            "Accuracy: 84.52%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "My improved approach predicted 31/49 words, which gives accuracy about 63%. Norvig solution gets 23/49, which give accuracy about 47%.\n",
        "It can be seen that introducing the keyboard layout as a factor leads to better results when fixing human typos. Most of the rest unfixed typos are just some words with typo what still occur to be existing word. I tried to generate candidates even for correct words, but it ends up replacing most of the other correct words, resulting in worse accuracy."
      ],
      "metadata": {
        "id": "mhJ-ASYzS1ht"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "accuracy on **random** tests"
      ],
      "metadata": {
        "id": "UfdMvwS0Rwn_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "good_text, bad_text = generate_texts(\"big.txt\",10000)\n",
        "\n",
        "print(\"original accuracy: \", end = \"\")\n",
        "compare_text(good_text,bad_text)\n",
        "print(\"\\nsolution accuracy: \",end = \"\")\n",
        "corrected_text = fix_typos(bad_text)\n",
        "compare_text(good_text,corrected_text)\n",
        "print(\"\\nNorvig accuracy: \",end = \"\")\n",
        "norvig_text = Norvig_fix_typos(bad_text)\n",
        "compare_text(good_text,norvig_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DHa3enlfRztK",
        "outputId": "025edee0-a61a-4a48-a8a7-ea5fc3ec176c"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "original accuracy: Correct words: 1590/1982\n",
            "Accuracy: 80.22%\n",
            "\n",
            "solution accuracy: Correct words: 1873/1982\n",
            "Accuracy: 94.50%\n",
            "\n",
            "Norvig accuracy: Correct words: 1846/1982\n",
            "Accuracy: 93.14%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "On random test the improvement is not so obvious, and its understandable, the typos here are randomly generated, not representing how humans are usually making them. I observed that in this case my approach either the same or just a tiny bit better."
      ],
      "metadata": {
        "id": "3LSCODlcVX6n"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uS0ZaWtvKopL"
      },
      "source": [
        "#### Useful resources (also included in the archive in moodle):\n",
        "\n",
        "1. [Possible dataset with N-grams](https://www.ngrams.info/download_coca.asp)\n",
        "2. [Damerau–Levenshtein distance](https://en.wikipedia.org/wiki/Damerau–Levenshtein_distance#:~:text=Informally%2C%20the%20Damerau–Levenshtein%20distance,one%20word%20into%20the%20other.)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}